jobs:
  - job: "Deploy"
    displayName: "Release: Pull Helm -> Deploy to EKS"
    continueOnError: false
    steps:
      - task: HelmInstaller@1
        displayName: 'Install Helm'
        inputs:
          helmVersionToInstall: ${{ parameters.helmVersion }}

      - task: AWSShellScript@1
        displayName: 'ECR login'
        inputs:
            awsCredentials: ${{ parameters.serviceConnectionName }}
            regionName: ${{ parameters.awsRegion }}
            
            # Sometimes the step is successfull, but the whole pipeline is failing. This is because
            # of this true. We need to set failOnStandardError to false to make it working.
            failOnStandardError: true
            scriptType: 'inline'
            inlineScript: |
              echo 'Using identity'
              aws sts get-caller-identity
              
              ## ECR Login
              export HELM_EXPERIMENTAL_OCI=1

              # Login with Helm
              echo 'HELM Login to ECR...'
              aws ecr get-login-password \
              --region $(awsECRRegion) | helm registry login \
              --username AWS \
              --password-stdin $(repositoryURI)

      - task: AWSShellScript@1
        displayName: 'Pull and deploy Helm Chart'
        inputs:
            awsCredentials: ${{ parameters.serviceConnectionName }}
            regionName: ${{ parameters.awsRegion }}
            failOnStandardError: true
            scriptType: 'inline'
            inlineScript: |
                # # Confirm caller identity
               
                # echo 'Using identity' - show who is accessing aws
                # aws sts get-caller-identity
                
                # Pull Helm Chart
                echo "Pulling helm chart from $(repositoryURI)"
                helm pull oci://$(repositoryURI) --version $(helmChartVersion)

                # Get Kubeconfig from the aws environment and put into the local kube config file
                echo 'Connect to Amazon EKS cluster'
                aws eks update-kubeconfig --region $(awsEKSRegion) --name ${{ parameters.awsEKSClusterName }}

                # INFO: Sometimes this file cannot pickup a valid role or credentials, so we have a permissions denied response from the AWS EKS
                # To prevent that, we can specify the role name. This is a hardcoded way. Need to find better way: maybe variables or 
                # add this role to the user we are using here.
                # aws eks update-kubeconfig --region $(awsEKSRegion) --name ${{ parameters.awsEKSClusterName }} --role-arn "arn:aws:iam::539590419140:role/hans.eks-access.role"
                
                # Sasha: remove this log ==========================
                # echo ============================================================
                # cat /home/vsts/.kube/config
                # echo ============================================================
                # echo "kubectl config current-context"
                # kubectl config current-context
                # echo "K8sNamespace: $(K8sNamespace)"

                # echo "kubectl get nodes -v=10"
                # kubectl get nodes -v=10

                # echo
                # echo "kubectl config view --minify"
                # kubectl config view --minify

                # echo "kubectl describe"
                # kubectl describe -n kube-system configmap/aws-auth
                # echo
                # echo "kubectl get svc"
                # kubectl get svc

                # echo
                # echo  "kubectl cluster-info dump"
                # kubectl cluster-info dump
                # echo

                # echo "kubectl cluster-info"
                # kubectl cluster-info
                # kubectl get clusterrolebindings
                # echo ============================================================
                # helm upgrade --install -f ./helm-chart/values.yaml azure-devops-test ./helm-chart/ --namespace azure-devops-test --create-namespace
                # Sasha: remove this log ==========================
                
                # Deploy Helm Chart
                echo 'Deploy Helm chart to EKS...'
                chartPackage=${{ parameters.projectName }}-$(helmChartVersion).tgz
                echo $(pwd)
                echo "chart package to be installed - $chartPackage"
                helm upgrade \
                  --namespace $(K8sNamespace) \
                  --create-namespace \
                  --install \
                  --wait \
                  --version $(helmChartVersion) \
                  --set application.name=${{ parameters.projectName }} \
                  --set image.repository=$(repositoryURI) \
                  --set appversion="$(Build.BuildNumber)" \
                  ${{ parameters.projectName }} \
                  $chartPackage \

      - task: AWSShellScript@1
        displayName: 'Display Kubectl & Helm Outputs'
        inputs:
          awsCredentials: ${{ parameters.serviceConnectionName }}
          regionName: ${{ parameters.awsRegion }}
          scriptType: 'inline'
          inlineScript: |
            echo "##[section]kubectl get all -n $(K8sNamespace)"
            kubectl get all -n $(K8sNamespace)

            echo "##[section]kubectl get deploy -n $(K8sNamespace)"
            kubectl get deploy -n $(K8sNamespace)

            echo "##[section]kubectl get services -n $(K8sNamespace)"
            kubectl get services -n $(K8sNamespace)

            echo "##[section]kubectl get pods -n $(K8sNamespace)"
            kubectl get pods -n $(K8sNamespace)

            echo "##[section]kubectl describe service webapp -n $(K8sNamespace)"
            kubectl describe service webapp -n $(K8sNamespace)

            echo "##[section]kubectl kubectl get secrets -n $(K8sNamespace)"
            kubectl get secrets -n $(K8sNamespace)

            echo "##[section]helm list -n $(K8sNamespace)"
            helm list -n $(K8sNamespace)

            echo "##[section]helm status ${{ parameters.projectName }} -n $(K8sNamespace)"
            helm status ${{ parameters.projectName }} -n $(K8sNamespace)
